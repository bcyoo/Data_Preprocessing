# -*- coding: utf-8 -*-
"""boston_pred_MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ftbtRk3DrMiKYZ2hPYBTjQiZ3TeaL4mn

4. 인공 신경망


    인공 신경망은 사람의 신경망을 모사하여 만든 예측 도구. 기본적으로 하나의 레이어에 다수의 노드를 가지고 있으며 
    여러 개의 레이어가 쌓인 신경망을 깊은 신경망이라고 한다. 이 때, 깊은 신경망을 이용하여 모델을 학습시키는 방법을 딥러닝이라고한다.
"""

from sklearn.datasets import load_boston
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split # 전체 데이터를 학습/평가 데이터로 나눔
from sklearn.preprocessing import MinMaxScaler # 데이터 내의 값을 0이상 1이하로값으로 조정

# ANN

import torch
from torch import nn, optim # torch 내의 세부적 기능을 불러온다 (신경망 기술, 손실함수, 최적화 방법)
from torch.utils.data import DataLoader, Dataset # 데이터를 모델에 사용할 수 잇또록 정리해주는 라이브러리
import torch.nn.functional as F # orch 내의 세부적 기능을 불러옴 (신경망)

# Loss
from sklearn.metrics import mean_squared_error # Regression 문제의 평가를 위해 MSE (Mean Squared Error)

# plot
import matplotlib.pyplot as plt

"""4.1 데이터 불러오기"""

bos = load_boston() # 보스턴 집값 데이터

# bos 내에는 data, feature name 등 다양항 정보 포함됨
df = pd.DataFrame(bos.data)  
df.columns = bos.feature_names # 데이터 변수명 불러오기
df['price'] = bos.target # 집 값에 해당하는 타겟 값 불러오기

df.head(10)

'''
CRIM : 범죄율
IMDUS : 비소매상업지역 면적 비율
NOX : 일산화질소 농도
RM : 주택당 방수
LSTAT : 인구 중 하위 계층 비율
B : 인구 중 흑인 비율
PTRATIO : 학생/교사 비율
ZM : 25,000 평방피트를 초과 거주지역 비율
CHAS : 찰스강의 경계에 위치한 경우 1, 아니면 0
AGE : 1940년 이전에 건축된 주택의 비율
RAD :  방사형 고속도로까지의 거리
DIS : 직업센터의 거리
TAX : 재산세율
'''

print('Description in Korean')

"""4.2 목표 성능 설정"""

# 우리가 만든 인공 신경망의 성능이 어느 정도인지 판단하기 위해 베이스라인 정함
# Multithreaded Local Learning Regulariztion Neural Networks for Regression Tasks, 2015
# 위 논문에서 보스턴 집 값 예측의 성능은 RMSE = 0.08019
# 따라서 실험 조건을 동일하게 하고 RMSE를 측정, 아래 정의돈 인공 신경망의 성능을 확힌해보자
print ('RMSE: 0.08019')

"""4.3 데이터 스케일링 (MinMax Scale)"""

df.info()

# 데이터를 넘바피 배열로 만들기
X = df.drop('price', axis=1).to_numpy() # 데이터 프레임에서 타겟값 (price)를 제외하고 np 배열로 만들기 axis=1은 열
Y = df['price'].to_numpy().reshape((-1,1)) # 데이터프레임 형태의 타겟값을 np 배열로 만들기

# 데이터 스케일링
# sklearn에서 제공되는 MinMaxscaler
# (x-min(x))/(max(x)-min(x))를 계산

scaler = MinMaxScaler() # 제일 큰 비율 1, 제일 작은 비율 0 으로해서 그 기준으로 데이터 값 변화.
scaler.fit(X)
X = scaler.transform(X)

scaler.fit(Y)
Y = scaler.transform(Y)

"""4.4 Tensor data와 batch 만들기"""

# tensor data로 변환하는 클래스

class TensorData(Dataset):

    def __init__(self, x_data, y_data):
        self.x_data = torch.FloatTensor(x_data)
        self.y_data = torch.FloatTensor(y_data)
        self.len = self.y_data.shape[0]

    def __getitem__(self, index):

        return self.x_data[index], self.y_data[index]

    def __len__(self):
        
        return self.len

## 전체 데이터를 학습/평가 데이터로 나눔
## 기준으로 잡은 논문이 5:5로 나눴기에 test_size를 0.5로 설정

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)

#학습 데이터, 시험 데이터 배치 형태로 구축
trainsets = TensorData(X_train, Y_train)
trainloader = torch.utils.data.DataLoader(trainsets, batch_size=32, shuffle=True)

testsets = TensorData(X_test, Y_test)
testloader = torch.utils.data.DataLoader(testsets, batch_size=32, shuffle=False)

"""4.5 모델 구축

    모델은 Regressor로 정의하며 입력층(노드13개), 2개의 은닉층(50,30개), 출력층(1개)로 구성한다. 
    데이터의 변수는 13개이므로 입력층의 노드는 13개가 되고 출력층은 집 값인 단일 값을 추출하는 것이므로 1개가 된다. 
    은닉층에 대해서는 실험을 하면서 튜닝할 수 있다.
"""

# 딥러닝에서는 모델을 자유롭게 만들수 있음 노드수, 레이어수를 원하는대로 설정가능
class Regressor(nn.Module):
    def __init__(self): # 모델정의
        super().__init__() # 모델 연산 정의
        self.fc1 = nn.Linear(13, 50, bias=True) # 입력층 (13) -> 은닉층1 (50)으로 가는연산
        self.fc2 = nn.Linear(50, 30, bias=True) # 은닉층1 (50) -> 은닉층2 (30)으로 가는연산
        self.fc3 = nn.Linear(30, 1, bias=True) # 은닉층 (30) -> 출력층 (1)로 가는연산
        self.dropout = nn.Dropout(0.2) # 연산이 될 때마다 20%의 비율로 랜덤하게 노드를 없앤다.

    def forward(self, x): # 모델 연산의 순서를 정의 # 실제 연산
        x = F.relu(self.fc1(x)) # Linear 계산 후 활성화 함수 ReLU를 적용   // input 13개 들어와서 relu를 거치고
        x = self.dropout(F.relu(self.fc2(x))) # 은닉층2에서 드랍아웃을 적용 (즉, 30개의 20%인 6개의 노드가 계산에서 제외한다.) // 은닉층에서 50개 들어와서 계산하고 relu를 지나감
        x = F.relu(self.fc3(x)) # Linear 계산 후 활성화 함수 Relu를 적용한다. // relu를 거쳐서 출력함.

        return x

    # 드랍아웃은 과적합(overfitting)을 방지하기 위해 노드의 일부를 배제하고 계산하는 방식이기때문에 출력층에 사용해서는 안됨.

"""4.6 모델, 손실함수, 최적화 방법 선언"""

model = Regressor() # Regressor 모델을 불러옴

criterion = nn.MSELoss()

# lr은 학습률
# weight_decay는 L2 정규화에서의 penalty 정도를 의미
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7) # 10e-7를 줌
# optim.Adam을 사용해서 model.parameters를 기준으로 최적화 (model에 있는 파라미터를 업데이트를 해야함)

"""4.7 학습진행"""

loss_ = [] # 그래프를 그리기 위한 loss 저장용 리스트

n = len(trainloader) # 배치 개수

for epoch in range(400): # 400번 학습
    
    running_loss = 0.0 # 배치마다 loss가 나옴 / 한번 학습 할때마다 쪼개지면서 학습하는데 평균을 내기위해 running_loss를 다 더한담에 마지막에 n으로 나눔

    for i, data in enumerate(trainloader, 0): # 무작위로 섞인 32개 데이터가 있는 배치가 하나씩 들어옴 # index를 뽑아내기위해 enumerate 사용

        inputs, values = data # data에는 X, Y가 들어있다.

        optimizer.zero_grad() # 최적화 초기화

        outputs = model(inputs) # 모델에 입력값 대입 후 예측 값 산출 / 13개 있는 데이터가 있는 변수가 32개 들어옴.
        loss = criterion(outputs, values) # 손실 함수 계산 /  32의 예측값과 집값이 나옴
        loss.backward() # 손실 함수 기준으로 역전파 설정
        optimizer.step() # 역전파를 진행하고 가중치 업데이트

        running_loss += loss.item() # epoch 마다 평균 loss를 계산하기 위해 loss를 더한다. /item을 써서 tensor를 변경하여 loss를 더함

    loss_.append(np.sqrt(running_loss) / n) # RMSE(Root Mean Squared Eroor) 계산

print('Finished Training')

plt.plot(loss_)
plt.title('Traing Loss')
plt.xlabel('epoch')
plt.show()

"""4.8 모델 평가"""

def evaluation(dataloader): # 평가 함수

    predictions = torch.tensor([], dtype=torch.float) # 예측값을 저장하는 텐서
    actual = torch.tensor([], dtype=torch.float) # 실제값을 저장하는 텐서

    with torch.no_grad(): # 평가할 때 grad 없이 계산
        model.eval() # 평가를 할 때에는 .eval() 반드시 사용해야함 / 평가를 할때에는 dropout 작용안해야하기 때문에 eval을 사용해서 계산
        for data in dataloader:
            inputs, values = data
            outputs = model(inputs)

            predictions = torch.cat((predictions, outputs), 0) # cat을 통해 예측값을 누적
            actual = torch.cat((actual, values), 0) # cat을 통해 실제값을 누적

    predictions = predictions.numpy() # 넘파이 배열로 변경
    actual = actual.numpy() # 넘파이 배열로 변경
    rmse = np.sqrt(mean_squared_error(predictions, actual)) # sklearn을 이용해 RMSE 계산

    return rmse

# 평가 시 .eval()을 사용하는 이유
# 평가 시에는 온전한 모델로 평가해야하는데 .eval()이 아닌 .train()인 경우 드랍아웃이 활성화 되어있다.
# 따라서 드랍아웃이나 배치 정규화 등과 같이 학습 시에만 사용하는 기술을 평가 시에는 비활성화 해야함.

train_rmse = evaluation(trainloader) # 학습 데이터 RMSE
test_rmse = evaluation(testloader) # 평가 데이터 RMSE

print('Train RMSE: ', train_rmse)
print('Test RMSE: ', test_rmse)

